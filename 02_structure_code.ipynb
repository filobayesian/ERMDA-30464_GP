{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Code Structuring\n",
        "\n",
        "This notebook structures the code to work with the actual data structure from `Data/Molise.dta`.\n",
        "\n",
        "## Key Findings from Phase 1:\n",
        "- **region_res**: 12 = Molise, 2 = Basilicata (need to check if Basilicata data exists)\n",
        "- **type**: 1 = Private, 2 = Public, 3 = Self-employed, 4 = Non-employed\n",
        "- **Years**: 1985-2019 (covers required 1997-2007 period)\n",
        "- **Key variables**: id_worker, year, wage, contract_type, sector_12cat, gender, year_birth\n",
        "\n",
        "## Tasks:\n",
        "1. Map actual variable names to analysis variables\n",
        "2. Create helper functions for data processing\n",
        "3. Design estimation wrappers\n",
        "4. Set up directory structure\n",
        "5. Create reusable modules in /src/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyreadstat\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Set up paths\n",
        "BASE_DIR = Path.cwd()\n",
        "DATA_DIR = BASE_DIR / \"Data\"\n",
        "SRC_DIR = BASE_DIR / \"src\"\n",
        "OUT_DIR = BASE_DIR / \"out\"\n",
        "DERIVED_DIR = BASE_DIR / \"data\" / \"derived\"\n",
        "\n",
        "# Create src directory if it doesn't exist\n",
        "SRC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Source directory: {SRC_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data to understand structure\n",
        "df, meta = pyreadstat.read_dta(DATA_DIR / \"Molise.dta\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"VARIABLE MAPPING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define variable mapping based on actual data\n",
        "VAR_MAP = {\n",
        "    # Identifiers\n",
        "    'person_id': 'id_worker',\n",
        "    'year': 'year',\n",
        "    'firm_id': 'id_firm',\n",
        "    \n",
        "    # Demographics\n",
        "    'gender': 'gender',  # 0=Male, 1=Female\n",
        "    'year_birth': 'year_birth',\n",
        "    'age': None,  # Will compute from year - year_birth\n",
        "    \n",
        "    # Geography\n",
        "    'region_residence': 'region_res',  # 12=Molise, 2=Basilicata\n",
        "    \n",
        "    # Worker classification\n",
        "    'worker_type_raw': 'type',  # 1=Private, 2=Public, 3=Self, 4=Non-employed\n",
        "    \n",
        "    # Employment\n",
        "    'contract_type': 'contract_type',  # 1=Permanent, 2=Temporary, 3=Seasonal\n",
        "    'sector': 'sector_12cat',\n",
        "    'occupation': 'occupation',\n",
        "    'firm_size': 'firm_dimension',\n",
        "    \n",
        "    # Outcomes\n",
        "    'wage': 'wage',  # Monthly wage (object type, needs conversion)\n",
        "    'working_weeks': 'working_weeks',  # Weeks worked\n",
        "    'part_time': 'part_time',\n",
        "    'part_time_fraction': 'part_time_fraction',\n",
        "    \n",
        "    # Dates\n",
        "    'date_start': 'date_start',\n",
        "    'date_end': 'date_end',\n",
        "}\n",
        "\n",
        "print(\"Variable mapping defined:\")\n",
        "for key, val in VAR_MAP.items():\n",
        "    print(f\"  {key}: {val if val else 'computed'}\")\n",
        "\n",
        "# Check if Basilicata data exists\n",
        "print(f\"\\n\\nRegion distribution:\")\n",
        "print(df['region_res'].value_counts())\n",
        "print(f\"\\nUnique regions: {df['region_res'].unique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the io.py module\n",
        "io_code = '''\"\"\"\n",
        "Data I/O functions for Molise earthquake analysis.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import pyreadstat\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_raw(data_file=None, base_dir=None):\n",
        "    \"\"\"\n",
        "    Load raw Stata data file.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data_file : str or Path, optional\n",
        "        Path to .dta file. If None, uses Data/Molise.dta relative to base_dir.\n",
        "    base_dir : str or Path, optional\n",
        "        Base directory. If None, uses current working directory.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    df : DataFrame\n",
        "        Loaded data\n",
        "    meta : dict\n",
        "        Metadata from Stata file (value labels, etc.)\n",
        "    \"\"\"\n",
        "    if base_dir is None:\n",
        "        base_dir = Path.cwd()\n",
        "    else:\n",
        "        base_dir = Path(base_dir)\n",
        "    \n",
        "    if data_file is None:\n",
        "        data_file = base_dir / \"Data\" / \"Molise.dta\"\n",
        "    else:\n",
        "        data_file = Path(data_file)\n",
        "    \n",
        "    try:\n",
        "        df, meta = pyreadstat.read_dta(data_file)\n",
        "        print(f\"Loaded {len(df):,} rows using pyreadstat\")\n",
        "    except Exception as e:\n",
        "        print(f\"pyreadstat failed: {e}, trying pandas...\")\n",
        "        df = pd.read_stata(data_file)\n",
        "        meta = None\n",
        "        print(f\"Loaded {len(df):,} rows using pandas\")\n",
        "    \n",
        "    return df, meta\n",
        "\n",
        "\n",
        "def write_parquet(df, path, partition_by=None):\n",
        "    \"\"\"\n",
        "    Write DataFrame to Parquet format.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Data to write\n",
        "    path : str or Path\n",
        "        Output path\n",
        "    partition_by : str, optional\n",
        "        Column name to partition by (e.g., 'year')\n",
        "    \"\"\"\n",
        "    path = Path(path)\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    if partition_by and partition_by in df.columns:\n",
        "        # Partition by specified column\n",
        "        for val in df[partition_by].unique():\n",
        "            subset = df[df[partition_by] == val]\n",
        "            part_path = path.parent / f\"{path.stem}_{partition_by}={val}{path.suffix}\"\n",
        "            subset.to_parquet(part_path, index=False)\n",
        "        print(f\"Wrote partitioned parquet files to {path.parent}\")\n",
        "    else:\n",
        "        df.to_parquet(path, index=False)\n",
        "        print(f\"Wrote parquet file to {path}\")\n",
        "\n",
        "\n",
        "def read_parquet(path):\n",
        "    \"\"\"Read Parquet file.\"\"\"\n",
        "    return pd.read_parquet(path)\n",
        "'''\n",
        "\n",
        "with open(SRC_DIR / \"io.py\", \"w\") as f:\n",
        "    f.write(io_code)\n",
        "print(\"Created src/io.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the build.py module\n",
        "build_code = '''\"\"\"\n",
        "Variable construction functions for Molise earthquake analysis.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import mstats\n",
        "\n",
        "\n",
        "def construct_worker_type(df, type_col='type'):\n",
        "    \"\"\"\n",
        "    Construct worker_type categorical variable.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Input data\n",
        "    type_col : str\n",
        "        Column name for raw worker type (1=Private, 2=Public, 3=Self, 4=Non-employed)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    Series\n",
        "        Categorical worker_type: 'private', 'public', 'self', 'non_employed'\n",
        "    \"\"\"\n",
        "    worker_type_map = {\n",
        "        1: 'private',\n",
        "        2: 'public',\n",
        "        3: 'self',\n",
        "        4: 'non_employed'\n",
        "    }\n",
        "    \n",
        "    worker_type = df[type_col].map(worker_type_map)\n",
        "    worker_type = pd.Categorical(worker_type, categories=['public', 'private', 'self', 'non_employed'], ordered=False)\n",
        "    \n",
        "    return worker_type\n",
        "\n",
        "\n",
        "def make_outcomes(df, wage_col='wage', contract_col='contract_type', \n",
        "                  working_weeks_col='working_weeks', year_col='year'):\n",
        "    \"\"\"\n",
        "    Construct outcome variables.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Input data\n",
        "    wage_col : str\n",
        "        Column name for wage\n",
        "    contract_col : str\n",
        "        Column name for contract type\n",
        "    working_weeks_col : str\n",
        "        Column name for working weeks\n",
        "    year_col : str\n",
        "        Column name for year\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "        DataFrame with outcome variables\n",
        "    \"\"\"\n",
        "    outcomes = pd.DataFrame(index=df.index)\n",
        "    \n",
        "    # Convert wage to numeric (handle object type)\n",
        "    if df[wage_col].dtype == 'object':\n",
        "        wage_numeric = pd.to_numeric(df[wage_col], errors='coerce')\n",
        "    else:\n",
        "        wage_numeric = df[wage_col]\n",
        "    \n",
        "    # Employment probability: 1 if has wage or working_weeks > 0\n",
        "    outcomes['emp_prob'] = (\n",
        "        (wage_numeric.notna() & (wage_numeric > 0)) | \n",
        "        (df[working_weeks_col].notna() & (pd.to_numeric(df[working_weeks_col], errors='coerce') > 0))\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Annualized monthly earnings (wage * 12, or wage * working_weeks/52 * 12)\n",
        "    if working_weeks_col in df.columns:\n",
        "        weeks = pd.to_numeric(df[working_weeks_col], errors='coerce').fillna(52)\n",
        "        earnings_annual = wage_numeric * (weeks / 52) * 12\n",
        "    else:\n",
        "        earnings_annual = wage_numeric * 12\n",
        "    \n",
        "    # Winsorize earnings at p1-p99\n",
        "    earnings_winsorized = earnings_annual.copy()\n",
        "    p1 = earnings_annual.quantile(0.01)\n",
        "    p99 = earnings_annual.quantile(0.99)\n",
        "    earnings_winsorized = earnings_winsorized.clip(lower=p1, upper=p99)\n",
        "    \n",
        "    # Inverse hyperbolic sine transformation\n",
        "    outcomes['earnings_asinh'] = np.arcsinh(earnings_winsorized)\n",
        "    \n",
        "    # Wage (daily equivalent if needed, or monthly)\n",
        "    outcomes['wage_asinh'] = np.arcsinh(wage_numeric.clip(lower=wage_numeric.quantile(0.01), \n",
        "                                                           upper=wage_numeric.quantile(0.99)))\n",
        "    \n",
        "    # Permanent contract indicator\n",
        "    if contract_col in df.columns:\n",
        "        # 1 = Permanent\n",
        "        outcomes['contract_perm'] = (pd.to_numeric(df[contract_col], errors='coerce') == 1).astype(int)\n",
        "    else:\n",
        "        outcomes['contract_perm'] = 0\n",
        "    \n",
        "    # Contract duration (if date_start and date_end available)\n",
        "    if 'date_start' in df.columns and 'date_end' in df.columns:\n",
        "        try:\n",
        "            date_start = pd.to_datetime(df['date_start'], errors='coerce')\n",
        "            date_end = pd.to_datetime(df['date_end'], errors='coerce')\n",
        "            outcomes['contract_duration_days'] = (date_end - date_start).dt.days\n",
        "            outcomes['contract_duration_days'] = outcomes['contract_duration_days'].fillna(0)\n",
        "        except:\n",
        "            outcomes['contract_duration_days'] = 0\n",
        "    else:\n",
        "        outcomes['contract_duration_days'] = 0\n",
        "    \n",
        "    return outcomes\n",
        "\n",
        "\n",
        "def make_flags(df, person_id_col='id_worker', year_col='year', \n",
        "               region_col='region_res', municipality_col=None):\n",
        "    \"\"\"\n",
        "    Create sample flags: balanced panel, stayers, border municipalities.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Input data\n",
        "    person_id_col : str\n",
        "        Person identifier column\n",
        "    year_col : str\n",
        "        Year column\n",
        "    region_col : str\n",
        "        Region column\n",
        "    municipality_col : str, optional\n",
        "        Municipality column (if available)\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "        DataFrame with flags\n",
        "    \"\"\"\n",
        "    flags = pd.DataFrame(index=df.index)\n",
        "    \n",
        "    # Balanced panel: present in all years 1997-2007 (excluding 2002)\n",
        "    required_years = set(range(1997, 2002)) | set(range(2003, 2008))\n",
        "    person_years = df.groupby(person_id_col)[year_col].apply(set)\n",
        "    flags['is_balanced_97_07'] = person_years.map(lambda x: required_years.issubset(x)).reindex(df[person_id_col]).values\n",
        "    \n",
        "    # Stayer: no inter-municipality moves (if municipality data available)\n",
        "    if municipality_col and municipality_col in df.columns:\n",
        "        person_municipalities = df.groupby(person_id_col)[municipality_col].nunique()\n",
        "        flags['is_stayer_residence'] = (person_municipalities == 1).reindex(df[person_id_col]).values\n",
        "    else:\n",
        "        flags['is_stayer_residence'] = True  # Assume all stayers if no municipality data\n",
        "    \n",
        "    # Border municipality (placeholder - would need actual border data)\n",
        "    flags['is_border_municipality'] = False\n",
        "    \n",
        "    return flags\n",
        "\n",
        "\n",
        "def assemble_panel(df, meta=None):\n",
        "    \"\"\"\n",
        "    Assemble analysis-ready panel from raw data.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Raw data\n",
        "    meta : dict, optional\n",
        "        Metadata from Stata file\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "        Analysis-ready panel\n",
        "    \"\"\"\n",
        "    panel = df.copy()\n",
        "    \n",
        "    # Compute age\n",
        "    if 'year_birth' in panel.columns:\n",
        "        panel['age'] = panel['year'] - panel['year_birth']\n",
        "        panel['age_sq'] = panel['age'] ** 2\n",
        "    else:\n",
        "        panel['age'] = np.nan\n",
        "        panel['age_sq'] = np.nan\n",
        "    \n",
        "    # Construct worker type\n",
        "    panel['worker_type'] = construct_worker_type(panel, type_col='type')\n",
        "    \n",
        "    # Filter to employed workers only (exclude type 4 = non-employed)\n",
        "    panel = panel[panel['worker_type'] != 'non_employed'].copy()\n",
        "    \n",
        "    # Treatment variables\n",
        "    # molise_res: 1 if region_res == 12 (Molise), 0 if region_res == 2 (Basilicata)\n",
        "    panel['molise_res'] = (panel['region_res'] == '12').astype(int)\n",
        "    \n",
        "    # post: 1 if year >= 2003\n",
        "    panel['post'] = (panel['year'] >= 2003).astype(int)\n",
        "    \n",
        "    # treat: interaction\n",
        "    panel['treat'] = panel['molise_res'] * panel['post']\n",
        "    \n",
        "    # event_time: year - 2002\n",
        "    panel['event_time'] = panel['year'] - 2002\n",
        "    \n",
        "    # Filter to analysis period: 1997-2001 and 2003-2007 (exclude 2002)\n",
        "    panel = panel[panel['year'].between(1997, 2007) & (panel['year'] != 2002)].copy()\n",
        "    \n",
        "    # Filter to Molise (12) or Basilicata (2) residents\n",
        "    panel = panel[panel['region_res'].isin(['12', '2'])].copy()\n",
        "    \n",
        "    # Make outcomes\n",
        "    outcomes = make_outcomes(panel)\n",
        "    for col in outcomes.columns:\n",
        "        panel[col] = outcomes[col]\n",
        "    \n",
        "    # Make flags\n",
        "    flags = make_flags(panel)\n",
        "    for col in flags.columns:\n",
        "        panel[col] = flags[col]\n",
        "    \n",
        "    # Fix molise_res based on pre-period (1997-2001) residence\n",
        "    pre_period = panel[panel['year'] < 2002].copy()\n",
        "    if len(pre_period) > 0:\n",
        "        pre_residence = pre_period.groupby('id_worker')['molise_res'].first()\n",
        "        panel['molise_res'] = pre_residence.reindex(panel['id_worker']).fillna(panel['molise_res']).values\n",
        "    \n",
        "    return panel\n",
        "'''\n",
        "\n",
        "with open(SRC_DIR / \"build.py\", \"w\") as f:\n",
        "    f.write(build_code)\n",
        "print(\"Created src/build.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the models.py module\n",
        "models_code = '''\"\"\"\n",
        "Estimation functions for DiD, DDD, and event-study models.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from linearmodels import PanelOLS\n",
        "from linearmodels.panel import compare\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "def did(df, outcome, cluster='id_worker', entity_effects=True, time_effects=True):\n",
        "    \"\"\"\n",
        "    Estimate difference-in-differences model.\n",
        "    \n",
        "    Model: Y = α + β(molise_res × post) + γ_i + λ_t + ε\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Panel data with entity_id and time_id\n",
        "    outcome : str\n",
        "        Outcome variable name\n",
        "    cluster : str\n",
        "        Variable to cluster standard errors by\n",
        "    entity_effects : bool\n",
        "        Include entity fixed effects\n",
        "    time_effects : bool\n",
        "        Include time fixed effects\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    result : PanelOLSResults\n",
        "        Estimation results\n",
        "    \"\"\"\n",
        "    # Prepare data\n",
        "    data = df.copy()\n",
        "    data = data.dropna(subset=[outcome, 'molise_res', 'post', 'treat'])\n",
        "    \n",
        "    # Create entity and time indices\n",
        "    data = data.set_index(['id_worker', 'year'])\n",
        "    \n",
        "    # Dependent variable\n",
        "    y = data[outcome]\n",
        "    \n",
        "    # Treatment variable\n",
        "    X = data[['treat']].copy()\n",
        "    \n",
        "    # Add constant\n",
        "    X = sm.add_constant(X)\n",
        "    \n",
        "    # Estimate\n",
        "    mod = PanelOLS(y, X, entity_effects=entity_effects, time_effects=time_effects)\n",
        "    result = mod.fit(cov_type='clustered', cluster_entity=True)\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def ddd_worker_type(df, outcome, cluster='id_worker', entity_effects=True, time_effects=True):\n",
        "    \"\"\"\n",
        "    Estimate triple-difference model by worker type.\n",
        "    \n",
        "    Model: Y = α + θ(molise_res × post × private) + φ(molise_res × post × self) + \n",
        "           all two-way interactions + γ_i + λ_t + μ_s + ε\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Panel data\n",
        "    outcome : str\n",
        "        Outcome variable name\n",
        "    cluster : str\n",
        "        Variable to cluster standard errors by\n",
        "    entity_effects : bool\n",
        "        Include entity fixed effects\n",
        "    time_effects : bool\n",
        "        Include time fixed effects\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    result : PanelOLSResults\n",
        "        Estimation results\n",
        "    \"\"\"\n",
        "    # Prepare data\n",
        "    data = df.copy()\n",
        "    data = data[data['worker_type'].isin(['public', 'private', 'self'])].copy()\n",
        "    data = data.dropna(subset=[outcome, 'molise_res', 'post', 'worker_type'])\n",
        "    \n",
        "    # Create worker type dummies\n",
        "    data['private'] = (data['worker_type'] == 'private').astype(int)\n",
        "    data['self'] = (data['worker_type'] == 'self').astype(int)\n",
        "    # public is baseline\n",
        "    \n",
        "    # Create triple interactions\n",
        "    data['treat_private'] = data['treat'] * data['private']\n",
        "    data['treat_self'] = data['treat'] * data['self']\n",
        "    \n",
        "    # Two-way interactions\n",
        "    data['molise_private'] = data['molise_res'] * data['private']\n",
        "    data['molise_self'] = data['molise_res'] * data['self']\n",
        "    data['post_private'] = data['post'] * data['private']\n",
        "    data['post_self'] = data['post'] * data['self']\n",
        "    \n",
        "    # Set index\n",
        "    data = data.set_index(['id_worker', 'year'])\n",
        "    \n",
        "    # Dependent variable\n",
        "    y = data[outcome]\n",
        "    \n",
        "    # Regressors: triple interactions and all two-way interactions\n",
        "    X = data[['treat_private', 'treat_self', \n",
        "              'molise_res', 'post', 'private', 'self',\n",
        "              'molise_private', 'molise_self', 'post_private', 'post_self']].copy()\n",
        "    \n",
        "    # Add constant\n",
        "    X = sm.add_constant(X)\n",
        "    \n",
        "    # Estimate with entity, time, and worker type FE\n",
        "    # Note: worker type FE via dummy variables in X\n",
        "    mod = PanelOLS(y, X, entity_effects=entity_effects, time_effects=time_effects)\n",
        "    result = mod.fit(cov_type='clustered', cluster_entity=True)\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def event_study(df, outcome, by_type=False, entity_effects=True, time_effects=True):\n",
        "    \"\"\"\n",
        "    Estimate event-study (dynamic DiD) model.\n",
        "    \n",
        "    Model: Y = α + Σ_{k≠-1} β_k[1{event_time=k} × molise_res] + γ_i + λ_t + ε\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Panel data\n",
        "    outcome : str\n",
        "        Outcome variable name\n",
        "    by_type : bool\n",
        "        If True, estimate separate dynamics by worker type\n",
        "    entity_effects : bool\n",
        "        Include entity fixed effects\n",
        "    time_effects : bool\n",
        "        Include time fixed effects\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    results_df : DataFrame\n",
        "        DataFrame with columns: event_time, beta, se, ci_low, ci_high\n",
        "    \"\"\"\n",
        "    data = df.copy()\n",
        "    data = data.dropna(subset=[outcome, 'molise_res', 'event_time'])\n",
        "    \n",
        "    # Filter to event_time in [-5, 5] (excluding 0 which is 2002)\n",
        "    data = data[data['event_time'].between(-5, 5) & (data['event_time'] != 0)].copy()\n",
        "    \n",
        "    # Create event time dummies (excluding -1 as reference)\n",
        "    event_times = sorted([k for k in data['event_time'].unique() if k != -1])\n",
        "    \n",
        "    results_list = []\n",
        "    \n",
        "    if by_type:\n",
        "        # Estimate separately by worker type\n",
        "        for worker_type in ['public', 'private', 'self']:\n",
        "            data_type = data[data['worker_type'] == worker_type].copy()\n",
        "            if len(data_type) == 0:\n",
        "                continue\n",
        "            \n",
        "            # Create interaction terms\n",
        "            for k in event_times:\n",
        "                data_type[f'event_{k}'] = ((data_type['event_time'] == k) * data_type['molise_res']).astype(int)\n",
        "            \n",
        "            # Set index\n",
        "            data_type = data_type.set_index(['id_worker', 'year'])\n",
        "            y = data_type[outcome]\n",
        "            \n",
        "            # Regressors: event time interactions\n",
        "            X_cols = [f'event_{k}' for k in event_times]\n",
        "            X = data_type[X_cols].copy()\n",
        "            X = sm.add_constant(X)\n",
        "            \n",
        "            # Estimate\n",
        "            mod = PanelOLS(y, X, entity_effects=entity_effects, time_effects=time_effects)\n",
        "            result = mod.fit(cov_type='clustered', cluster_entity=True)\n",
        "            \n",
        "            # Extract coefficients\n",
        "            for k in event_times:\n",
        "                coef_name = f'event_{k}'\n",
        "                if coef_name in result.params.index:\n",
        "                    beta = result.params[coef_name]\n",
        "                    se = result.std_errors[coef_name]\n",
        "                    ci_low = beta - 1.96 * se\n",
        "                    ci_high = beta + 1.96 * se\n",
        "                    results_list.append({\n",
        "                        'event_time': k,\n",
        "                        'worker_type': worker_type,\n",
        "                        'beta': beta,\n",
        "                        'se': se,\n",
        "                        'ci_low': ci_low,\n",
        "                        'ci_high': ci_high\n",
        "                    })\n",
        "            \n",
        "            # Add reference period (-1)\n",
        "            results_list.append({\n",
        "                'event_time': -1,\n",
        "                'worker_type': worker_type,\n",
        "                'beta': 0.0,\n",
        "                'se': 0.0,\n",
        "                'ci_low': 0.0,\n",
        "                'ci_high': 0.0\n",
        "            })\n",
        "    else:\n",
        "        # Pooled estimation\n",
        "        # Create interaction terms\n",
        "        for k in event_times:\n",
        "            data[f'event_{k}'] = ((data['event_time'] == k) * data['molise_res']).astype(int)\n",
        "        \n",
        "        # Set index\n",
        "        data = data.set_index(['id_worker', 'year'])\n",
        "        y = data[outcome]\n",
        "        \n",
        "        # Regressors\n",
        "        X_cols = [f'event_{k}' for k in event_times]\n",
        "        X = data[X_cols].copy()\n",
        "        X = sm.add_constant(X)\n",
        "        \n",
        "        # Estimate\n",
        "        mod = PanelOLS(y, X, entity_effects=entity_effects, time_effects=time_effects)\n",
        "        result = mod.fit(cov_type='clustered', cluster_entity=True)\n",
        "        \n",
        "        # Extract coefficients\n",
        "        for k in event_times:\n",
        "            coef_name = f'event_{k}'\n",
        "            if coef_name in result.params.index:\n",
        "                beta = result.params[coef_name]\n",
        "                se = result.std_errors[coef_name]\n",
        "                ci_low = beta - 1.96 * se\n",
        "                ci_high = beta + 1.96 * se\n",
        "                results_list.append({\n",
        "                    'event_time': k,\n",
        "                    'beta': beta,\n",
        "                    'se': se,\n",
        "                    'ci_low': ci_low,\n",
        "                    'ci_high': ci_high\n",
        "                })\n",
        "        \n",
        "        # Add reference period\n",
        "        results_list.append({\n",
        "            'event_time': -1,\n",
        "            'beta': 0.0,\n",
        "            'se': 0.0,\n",
        "            'ci_low': 0.0,\n",
        "            'ci_high': 0.0\n",
        "        })\n",
        "    \n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    return results_df\n",
        "'''\n",
        "\n",
        "with open(SRC_DIR / \"models.py\", \"w\") as f:\n",
        "    f.write(models_code)\n",
        "print(\"Created src/models.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create plots.py, export.py, robustness.py, and diagnostics.py modules\n",
        "# (Creating stubs for now, will be expanded in Phase 3)\n",
        "\n",
        "plots_code = '''\"\"\"\n",
        "Plotting functions for Molise earthquake analysis.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def plot_event(results_df, title, outfile, by_type=False):\n",
        "    \"\"\"\n",
        "    Plot event-study coefficients with confidence intervals.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    results_df : DataFrame\n",
        "        Results from event_study()\n",
        "    title : str\n",
        "        Plot title\n",
        "    outfile : str or Path\n",
        "        Output file path\n",
        "    by_type : bool\n",
        "        If True, plot separate lines for each worker type\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    if by_type and 'worker_type' in results_df.columns:\n",
        "        for worker_type in results_df['worker_type'].unique():\n",
        "            data = results_df[results_df['worker_type'] == worker_type].sort_values('event_time')\n",
        "            ax.plot(data['event_time'], data['beta'], marker='o', label=worker_type)\n",
        "            ax.fill_between(data['event_time'], data['ci_low'], data['ci_high'], alpha=0.2)\n",
        "    else:\n",
        "        data = results_df.sort_values('event_time')\n",
        "        ax.plot(data['event_time'], data['beta'], marker='o', color='blue')\n",
        "        ax.fill_between(data['event_time'], data['ci_low'], data['ci_high'], alpha=0.2)\n",
        "    \n",
        "    ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
        "    ax.axvline(x=0, color='red', linestyle='--', linewidth=0.5, label='Earthquake')\n",
        "    ax.set_xlabel('Event Time (Years from 2002)')\n",
        "    ax.set_ylabel('Coefficient')\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    outfile = Path(outfile)\n",
        "    outfile.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(outfile, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Saved plot to {outfile}\")\n",
        "\n",
        "\n",
        "def plot_three_env(tables, outfile):\n",
        "    \"\"\"Plot three environments analysis.\"\"\"\n",
        "    # Placeholder - will implement in Phase 3\n",
        "    pass\n",
        "\n",
        "\n",
        "def plot_heatmap(grid, outfile):\n",
        "    \"\"\"Plot coefficient heatmap for heterogeneity analysis.\"\"\"\n",
        "    # Placeholder - will implement in Phase 3\n",
        "    pass\n",
        "'''\n",
        "\n",
        "export_code = '''\"\"\"\n",
        "Export functions for tables and summaries.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def write_table(result, outfile, format='both'):\n",
        "    \"\"\"\n",
        "    Write estimation results to CSV and/or LaTeX.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    result : PanelOLSResults or similar\n",
        "        Estimation results\n",
        "    outfile : str or Path\n",
        "        Output file path (without extension)\n",
        "    format : str\n",
        "        'csv', 'tex', or 'both'\n",
        "    \"\"\"\n",
        "    outfile = Path(outfile)\n",
        "    outfile.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Extract coefficients and standard errors\n",
        "    summary = pd.DataFrame({\n",
        "        'Coefficient': result.params,\n",
        "        'Std_Error': result.std_errors,\n",
        "        'P_Value': result.pvalues\n",
        "    })\n",
        "    \n",
        "    if 'csv' in format or format == 'both':\n",
        "        summary.to_csv(f\"{outfile}.csv\")\n",
        "        print(f\"Saved table to {outfile}.csv\")\n",
        "    \n",
        "    if 'tex' in format or format == 'both':\n",
        "        summary.to_latex(f\"{outfile}.tex\", float_format=\"%.4f\")\n",
        "        print(f\"Saved table to {outfile}.tex\")\n",
        "\n",
        "\n",
        "def write_summary(results, outfile):\n",
        "    \"\"\"Write summary of multiple results.\"\"\"\n",
        "    # Placeholder - will implement in Phase 3\n",
        "    pass\n",
        "'''\n",
        "\n",
        "robustness_code = '''\"\"\"\n",
        "Robustness check functions.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from .models import did\n",
        "\n",
        "\n",
        "def run_all(df):\n",
        "    \"\"\"\n",
        "    Run all robustness checks.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary of results keyed by robustness check name\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    # Placeholder - will implement in Phase 3\n",
        "    return results\n",
        "'''\n",
        "\n",
        "diagnostics_code = '''\"\"\"\n",
        "Diagnostic functions.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def pretrend_test(event_df):\n",
        "    \"\"\"\n",
        "    Test for parallel trends in pre-period.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    event_df : DataFrame\n",
        "        Event-study results\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Test statistics and p-values\n",
        "    \"\"\"\n",
        "    # Filter to pre-period (event_time < 0)\n",
        "    pre = event_df[event_df['event_time'] < 0].copy()\n",
        "    \n",
        "    if len(pre) == 0:\n",
        "        return {'test_stat': np.nan, 'p_value': np.nan}\n",
        "    \n",
        "    # Test if sum of pre-period coefficients is zero\n",
        "    # This is a simplified version - full implementation would use F-test\n",
        "    coefs = pre['beta'].values\n",
        "    ses = pre['se'].values\n",
        "    \n",
        "    # Wald test\n",
        "    test_stat = (coefs.sum() ** 2) / (ses ** 2).sum()\n",
        "    p_value = 1 - stats.chi2.cdf(test_stat, df=len(coefs))\n",
        "    \n",
        "    return {'test_stat': test_stat, 'p_value': p_value, 'n_pre_periods': len(pre)}\n",
        "\n",
        "\n",
        "def cell_counts(df, keys):\n",
        "    \"\"\"\n",
        "    Count observations by cell (e.g., region × type × year).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Data\n",
        "    keys : list of str\n",
        "        Column names to group by\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "        Cell counts\n",
        "    \"\"\"\n",
        "    counts = df.groupby(keys).size().reset_index(name='count')\n",
        "    return counts\n",
        "'''\n",
        "\n",
        "# Write all modules\n",
        "modules = {\n",
        "    'plots.py': plots_code,\n",
        "    'export.py': export_code,\n",
        "    'robustness.py': robustness_code,\n",
        "    'diagnostics.py': diagnostics_code\n",
        "}\n",
        "\n",
        "for filename, code in modules.items():\n",
        "    with open(SRC_DIR / filename, \"w\") as f:\n",
        "        f.write(code)\n",
        "    print(f\"Created src/{filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test loading and basic construction\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTING MODULES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from src.io import load_raw\n",
        "from src.build import assemble_panel, construct_worker_type\n",
        "\n",
        "# Load data\n",
        "df_raw, meta = load_raw()\n",
        "print(f\"\\nLoaded {len(df_raw):,} rows\")\n",
        "\n",
        "# Check if we have Basilicata data\n",
        "print(f\"\\nRegion distribution in raw data:\")\n",
        "print(df_raw['region_res'].value_counts())\n",
        "\n",
        "# Try to assemble panel (will filter to available regions)\n",
        "try:\n",
        "    panel = assemble_panel(df_raw, meta)\n",
        "    print(f\"\\nAssembled panel: {len(panel):,} rows\")\n",
        "    print(f\"Years: {panel['year'].min()} - {panel['year'].max()}\")\n",
        "    print(f\"Worker types: {panel['worker_type'].value_counts().to_dict()}\")\n",
        "    print(f\"Molise vs Basilicata: {panel['molise_res'].value_counts().to_dict()}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError assembling panel: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Code structure created with:\n",
        "- `src/io.py`: Data loading functions\n",
        "- `src/build.py`: Variable construction functions\n",
        "- `src/models.py`: Estimation functions (DiD, DDD, event-study)\n",
        "- `src/plots.py`: Plotting functions\n",
        "- `src/export.py`: Table export functions\n",
        "- `src/robustness.py`: Robustness checks\n",
        "- `src/diagnostics.py`: Diagnostic functions\n",
        "\n",
        "**Note**: If Basilicata data is not in the file, we may need to:\n",
        "1. Load additional data files, or\n",
        "2. Use a different control region (e.g., Puglia = 14), or\n",
        "3. Use only within-Molise variation (private vs public workers)\n",
        "\n",
        "Proceeding to Phase 3 with the current structure.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
