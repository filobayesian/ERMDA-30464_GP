{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: Data Analysis\n",
        "\n",
        "This notebook analyzes the structure of `Data/Molise.dta` to understand:\n",
        "- Available variables and their types\n",
        "- Temporal coverage\n",
        "- Region distribution\n",
        "- Worker classification fields\n",
        "- Outcome variables\n",
        "- Data quality issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyreadstat\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set up paths\n",
        "BASE_DIR = Path.cwd()\n",
        "DATA_DIR = BASE_DIR / \"Data\"\n",
        "OUT_DIR = BASE_DIR / \"out\"\n",
        "DERIVED_DIR = BASE_DIR / \"data\" / \"derived\"\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Data file exists: {(DATA_DIR / 'Molise.dta').exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Stata files (both Molise and Basilicata if available)\n",
        "molise_file = DATA_DIR / \"Molise.dta\"\n",
        "basilicata_file = DATA_DIR / \"Basilicata.dta\"\n",
        "\n",
        "dfs = []\n",
        "meta = None\n",
        "\n",
        "# Load Molise\n",
        "if molise_file.exists():\n",
        "    try:\n",
        "        df_molise, meta = pyreadstat.read_dta(molise_file)\n",
        "        df_molise['region_res'] = '12'  # Ensure region code\n",
        "        dfs.append(df_molise)\n",
        "        print(f\"Loaded Molise: {len(df_molise):,} rows using pyreadstat\")\n",
        "    except Exception as e:\n",
        "        print(f\"pyreadstat failed for Molise: {e}, trying pandas...\")\n",
        "        df_molise = pd.read_stata(molise_file)\n",
        "        df_molise['region_res'] = '12'\n",
        "        dfs.append(df_molise)\n",
        "        print(f\"Loaded Molise: {len(df_molise):,} rows using pandas\")\n",
        "else:\n",
        "    print(f\"Warning: {molise_file} not found\")\n",
        "\n",
        "# Load Basilicata if available\n",
        "if basilicata_file.exists():\n",
        "    try:\n",
        "        df_basilicata, meta_bas = pyreadstat.read_dta(basilicata_file)\n",
        "        df_basilicata['region_res'] = '2'  # Ensure region code\n",
        "        dfs.append(df_basilicata)\n",
        "        print(f\"Loaded Basilicata: {len(df_basilicata):,} rows using pyreadstat\")\n",
        "    except Exception as e:\n",
        "        print(f\"pyreadstat failed for Basilicata: {e}, trying pandas...\")\n",
        "        df_basilicata = pd.read_stata(basilicata_file)\n",
        "        df_basilicata['region_res'] = '2'\n",
        "        dfs.append(df_basilicata)\n",
        "        print(f\"Loaded Basilicata: {len(df_basilicata):,} rows using pandas\")\n",
        "else:\n",
        "    print(f\"Note: {basilicata_file} not found - analyzing Molise only\")\n",
        "\n",
        "# Combine if both loaded\n",
        "if len(dfs) > 1:\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"\\nCombined dataset: {len(df):,} rows\")\n",
        "elif len(dfs) == 1:\n",
        "    df = dfs[0]\n",
        "    print(f\"\\nSingle dataset: {len(df):,} rows\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"No data files found\")\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic info about the dataset\n",
        "print(\"=\" * 80)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nShape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=\" * 80)\n",
        "print(\"MISSING VALUES\")\n",
        "print(\"=\" * 80)\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (missing / len(df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Missing %': missing_pct\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "print(missing_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify key variables - look for common patterns\n",
        "print(\"=\" * 80)\n",
        "print(\"KEY VARIABLE IDENTIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Person identifier\n",
        "person_id_cols = [c for c in df.columns if any(x in c.lower() for x in ['id', 'person', 'individual', 'codice'])]\n",
        "print(f\"\\nPotential person ID columns: {person_id_cols}\")\n",
        "\n",
        "# Year\n",
        "year_cols = [c for c in df.columns if any(x in c.lower() for x in ['year', 'anno', 'yr'])]\n",
        "print(f\"\\nPotential year columns: {year_cols}\")\n",
        "\n",
        "# Region\n",
        "region_cols = [c for c in df.columns if any(x in c.lower() for x in ['region', 'regione', 'reg'])]\n",
        "print(f\"\\nPotential region columns: {region_cols}\")\n",
        "\n",
        "# Municipality\n",
        "municipality_cols = [c for c in df.columns if any(x in c.lower() for x in ['municip', 'comune', 'city'])]\n",
        "print(f\"\\nPotential municipality columns: {municipality_cols}\")\n",
        "\n",
        "# Gender\n",
        "gender_cols = [c for c in df.columns if any(x in c.lower() for x in ['gender', 'sex', 'sesso', 'genere'])]\n",
        "print(f\"\\nPotential gender columns: {gender_cols}\")\n",
        "\n",
        "# Age\n",
        "age_cols = [c for c in df.columns if any(x in c.lower() for x in ['age', 'eta', 'anni'])]\n",
        "print(f\"\\nPotential age columns: {age_cols}\")\n",
        "\n",
        "# Employment/Earnings\n",
        "employment_cols = [c for c in df.columns if any(x in c.lower() for x in ['employ', 'earn', 'wage', 'reddito', 'stipendio', 'lavoro'])]\n",
        "print(f\"\\nPotential employment/earnings columns: {employment_cols}\")\n",
        "\n",
        "# Contract\n",
        "contract_cols = [c for c in df.columns if any(x in c.lower() for x in ['contract', 'contratto', 'tipologia'])]\n",
        "print(f\"\\nPotential contract columns: {contract_cols}\")\n",
        "\n",
        "# Sector\n",
        "sector_cols = [c for c in df.columns if any(x in c.lower() for x in ['sector', 'settore', 'nace', 'ateco'])]\n",
        "print(f\"\\nPotential sector columns: {sector_cols}\")\n",
        "\n",
        "# Public/Private\n",
        "worker_type_cols = [c for c in df.columns if any(x in c.lower() for x in ['public', 'private', 'self', 'pubblico', 'privato', 'autonomo', 'dipendente'])]\n",
        "print(f\"\\nPotential worker type columns: {worker_type_cols}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"=\" * 80)\n",
        "print(\"SAMPLE DATA (first 5 rows)\")\n",
        "print(\"=\" * 80)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check temporal coverage\n",
        "if year_cols:\n",
        "    year_col = year_cols[0]\n",
        "    print(\"=\" * 80)\n",
        "    print(\"TEMPORAL COVERAGE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nYear column: {year_col}\")\n",
        "    print(f\"\\nYear range: {df[year_col].min()} - {df[year_col].max()}\")\n",
        "    print(f\"\\nYear distribution:\")\n",
        "    print(df[year_col].value_counts().sort_index())\n",
        "else:\n",
        "    print(\"No year column identified - need to inspect manually\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check region distribution\n",
        "if region_cols:\n",
        "    region_col = region_cols[0]\n",
        "    print(\"=\" * 80)\n",
        "    print(\"REGION DISTRIBUTION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nRegion column: {region_col}\")\n",
        "    print(f\"\\nUnique regions: {df[region_col].nunique()}\")\n",
        "    print(f\"\\nRegion distribution:\")\n",
        "    print(df[region_col].value_counts())\n",
        "    \n",
        "    # Check for Molise and Basilicata specifically\n",
        "    if df[region_col].dtype == 'object':\n",
        "        molise_check = df[region_col].str.contains('Molise|molise', case=False, na=False).sum()\n",
        "        basilicata_check = df[region_col].str.contains('Basilicata|basilicata', case=False, na=False).sum()\n",
        "        print(f\"\\nRows containing 'Molise': {molise_check}\")\n",
        "        print(f\"\\nRows containing 'Basilicata': {basilicata_check}\")\n",
        "    else:\n",
        "        print(f\"\\nRegion values (first 20): {df[region_col].unique()[:20]}\")\n",
        "else:\n",
        "    print(\"No region column identified - need to inspect manually\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed variable inspection - show unique values for categorical variables\n",
        "print(\"=\" * 80)\n",
        "print(\"DETAILED VARIABLE INSPECTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# For each column, show basic stats\n",
        "for col in df.columns:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Type: {df[col].dtype}\")\n",
        "    print(f\"  Non-null: {df[col].notna().sum():,} / {len(df):,}\")\n",
        "    if df[col].dtype in ['object', 'category']:\n",
        "        n_unique = df[col].nunique()\n",
        "        print(f\"  Unique values: {n_unique}\")\n",
        "        if n_unique <= 20:\n",
        "            print(f\"  Values: {df[col].unique()}\")\n",
        "        else:\n",
        "            print(f\"  First 10 values: {df[col].unique()[:10]}\")\n",
        "    elif df[col].dtype in ['int64', 'float64']:\n",
        "        print(f\"  Min: {df[col].min()}, Max: {df[col].max()}, Mean: {df[col].mean():.2f}\")\n",
        "        if df[col].nunique() <= 20:\n",
        "            print(f\"  Unique values: {sorted(df[col].dropna().unique())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a codebook/data dictionary\n",
        "print(\"=\" * 80)\n",
        "print(\"CREATING CODEBOOK\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "codebook = []\n",
        "for col in df.columns:\n",
        "    entry = {\n",
        "        'Variable': col,\n",
        "        'Type': str(df[col].dtype),\n",
        "        'Non_Missing': df[col].notna().sum(),\n",
        "        'Missing': df[col].isna().sum(),\n",
        "        'Missing_Pct': f\"{(df[col].isna().sum() / len(df) * 100):.2f}%\",\n",
        "        'N_Unique': df[col].nunique()\n",
        "    }\n",
        "    \n",
        "    if df[col].dtype in ['int64', 'float64']:\n",
        "        entry['Min'] = df[col].min()\n",
        "        entry['Max'] = df[col].max()\n",
        "        entry['Mean'] = f\"{df[col].mean():.2f}\"\n",
        "        entry['Median'] = f\"{df[col].median():.2f}\"\n",
        "    else:\n",
        "        entry['Min'] = 'N/A'\n",
        "        entry['Max'] = 'N/A'\n",
        "        entry['Mean'] = 'N/A'\n",
        "        entry['Median'] = 'N/A'\n",
        "    \n",
        "    if df[col].nunique() <= 50:\n",
        "        unique_vals = df[col].dropna().unique()\n",
        "        if len(unique_vals) <= 20:\n",
        "            entry['Sample_Values'] = str(list(unique_vals))\n",
        "        else:\n",
        "            entry['Sample_Values'] = str(list(unique_vals[:20])) + \" ...\"\n",
        "    else:\n",
        "        entry['Sample_Values'] = f\"{df[col].nunique()} unique values\"\n",
        "    \n",
        "    codebook.append(entry)\n",
        "\n",
        "codebook_df = pd.DataFrame(codebook)\n",
        "print(codebook_df.to_string())\n",
        "\n",
        "# Save codebook\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "codebook_df.to_csv(OUT_DIR / \"codebook.csv\", index=False)\n",
        "print(f\"\\n\\nCodebook saved to: {OUT_DIR / 'codebook.csv'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for key numeric variables\n",
        "print(\"=\" * 80)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "if len(numeric_cols) > 0:\n",
        "    print(df[numeric_cols].describe())\n",
        "else:\n",
        "    print(\"No numeric columns found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save a sample of the data for inspection\n",
        "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "df_sample = df.head(1000)\n",
        "df_sample.to_parquet(DERIVED_DIR / \"data_sample.parquet\", index=False)\n",
        "print(f\"Sample data saved to: {DERIVED_DIR / 'data_sample.parquet'}\")\n",
        "print(f\"\\nAnalysis complete! Review the codebook and variable inspection above.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
