{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: Data Analysis\n",
        "\n",
        "This notebook analyzes the structure of `Data/Molise.dta` to understand:\n",
        "- Available variables and their types\n",
        "- Temporal coverage\n",
        "- Region distribution\n",
        "- Worker classification fields\n",
        "- Outcome variables\n",
        "- Data quality issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base directory: /Users/vittoriogaravelli/GitHub/GitHub/ERMDA-30464_GP\n",
            "Data directory: /Users/vittoriogaravelli/GitHub/GitHub/ERMDA-30464_GP/Data\n",
            "Data file exists: True\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyreadstat\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set up paths\n",
        "BASE_DIR = Path.cwd()\n",
        "DATA_DIR = BASE_DIR / \"Data\"\n",
        "OUT_DIR = BASE_DIR / \"out\"\n",
        "DERIVED_DIR = BASE_DIR / \"data\" / \"derived\"\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Data file exists: {(DATA_DIR / 'Molise.dta').exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded Molise: 298,889 rows using pyreadstat\n",
            "Loaded Basilicata: 555,155 rows using pyreadstat\n",
            "Note: /Users/vittoriogaravelli/GitHub/GitHub/ERMDA-30464_GP/Data/Puglia.dta not found - skipping Puglia\n",
            "\n",
            "Combined dataset: 854,044 rows\n",
            "\n",
            "Dataset shape: (854044, 24)\n",
            "Memory usage: 541.06 MB\n"
          ]
        }
      ],
      "source": [
        "# Load the Stata files (Molise, Basilicata, and Puglia if available)\n",
        "molise_file = DATA_DIR / \"Molise.dta\"\n",
        "basilicata_file = DATA_DIR / \"Basilicata.dta\"\n",
        "puglia_file = DATA_DIR / \"Puglia.dta\"\n",
        "\n",
        "region_specs = [\n",
        "    (\"Molise\", molise_file, \"12\"),\n",
        "    (\"Basilicata\", basilicata_file, \"2\"),\n",
        "    (\"Puglia\", puglia_file, \"16\"),\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "meta = None\n",
        "\n",
        "for region_name, path, region_code in region_specs:\n",
        "    if not path.exists():\n",
        "        print(f\"Note: {path} not found - skipping {region_name}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df_region, meta_region = pyreadstat.read_dta(path)\n",
        "        print(f\"Loaded {region_name}: {len(df_region):,} rows using pyreadstat\")\n",
        "    except Exception as e:\n",
        "        print(f\"pyreadstat failed for {region_name}: {e}, trying pandas...\")\n",
        "        df_region = pd.read_stata(path)\n",
        "        meta_region = None\n",
        "        print(f\"Loaded {region_name}: {len(df_region):,} rows using pandas\")\n",
        "\n",
        "    if meta is None and meta_region is not None:\n",
        "        meta = meta_region\n",
        "\n",
        "    df_region[\"region_res\"] = region_code\n",
        "    dfs.append(df_region)\n",
        "\n",
        "if not dfs:\n",
        "    raise FileNotFoundError(\"No data files found\")\n",
        "\n",
        "if len(dfs) > 1:\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"\\nCombined dataset: {len(df):,} rows\")\n",
        "else:\n",
        "    df = dfs[0]\n",
        "    print(f\"\\nSingle dataset: {len(df):,} rows\")\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DATASET OVERVIEW\n",
            "================================================================================\n",
            "\n",
            "Shape: 854,044 rows × 24 columns\n",
            "\n",
            "Column names:\n",
            "['id_worker', 'year', 'year_birth', 'year_death', 'gender', 'region_res', 'year_pension', 'type', 'id_firm', 'date_start', 'date_end', 'working_weeks', 'wage', 'reason_end', 'part_time', 'part_time_fraction', 'contract_type', 'occupation', 'incentive_policy', 'firm_dimension', 'sector_2d', 'sector_12cat', 'firm_position', 'id_firm_parent']\n",
            "\n",
            "Data types:\n",
            "id_worker               int64\n",
            "year                    int64\n",
            "year_birth              int64\n",
            "year_death             object\n",
            "gender                  int64\n",
            "region_res             object\n",
            "year_pension           object\n",
            "type                    int64\n",
            "id_firm                object\n",
            "date_start             object\n",
            "date_end               object\n",
            "working_weeks          object\n",
            "wage                   object\n",
            "reason_end             object\n",
            "part_time              object\n",
            "part_time_fraction    float64\n",
            "contract_type          object\n",
            "occupation             object\n",
            "incentive_policy       object\n",
            "firm_dimension         object\n",
            "sector_2d              object\n",
            "sector_12cat           object\n",
            "firm_position          object\n",
            "id_firm_parent         object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Basic info about the dataset\n",
        "print(\"=\" * 80)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nShape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SAMPLE DATA (first 5 rows)\n",
            "================================================================================\n",
            "   id_worker  year  year_birth year_death  gender region_res year_pension  \\\n",
            "0       3052  2005        1981        NaN       0         12          NaN   \n",
            "1       3052  2002        1981        NaN       0         12          NaN   \n",
            "2       3052  2017        1981        NaN       0         12          NaN   \n",
            "3       3052  2016        1981        NaN       0         12          NaN   \n",
            "4       3052  2008        1981        NaN       0         12          NaN   \n",
            "\n",
            "   type  id_firm  date_start date_end working_weeks   wage reason_end  \\\n",
            "0     1  3583834  2005-12-27      NaN             1    300        NaN   \n",
            "1     4      NaN         NaN      NaN           NaN    NaN        NaN   \n",
            "2     1  2226770         NaN      NaN            52  19100        NaN   \n",
            "3     1  2226770         NaN      NaN            52  18100        NaN   \n",
            "4     1  5743594  2008-01-24      NaN            50  18700        NaN   \n",
            "\n",
            "  part_time  part_time_fraction contract_type occupation incentive_policy  \\\n",
            "0         0                 NaN             2          4                9   \n",
            "1       NaN                 NaN           NaN        NaN              NaN   \n",
            "2         0                 NaN             1          4               51   \n",
            "3         0                 NaN             1          4               51   \n",
            "4         0                 NaN             2          4              NaN   \n",
            "\n",
            "  firm_dimension sector_2d sector_12cat firm_position id_firm_parent  \n",
            "0             14        78            7             2        3583834  \n",
            "1            NaN       NaN          NaN           NaN            NaN  \n",
            "2              2        49           10             3        2226770  \n",
            "3              2        49           10             3        2226770  \n",
            "4             14        29            5             2        5743594  \n"
          ]
        }
      ],
      "source": [
        "# Display first few rows\n",
        "print(\"=\" * 80)\n",
        "print(\"SAMPLE DATA (first 5 rows)\")\n",
        "print(\"=\" * 80)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['sector_firm'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m mask = region_series == region_code\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     subset = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_keep\u001b[49m\u001b[43m]\u001b[49m.copy()\n\u001b[32m     20\u001b[39m     filtered_subsets[region_name] = subset\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion_name.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m filtered shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data_analysis/lib/python3.12/site-packages/pandas/core/indexing.py:1184\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m   1183\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._get_value(*key, takeable=\u001b[38;5;28mself\u001b[39m._takeable)\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1186\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[32m   1187\u001b[39m     axis = \u001b[38;5;28mself\u001b[39m.axis \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data_analysis/lib/python3.12/site-packages/pandas/core/indexing.py:1377\u001b[39m, in \u001b[36m_LocIndexer._getitem_tuple\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_take_opportunity(tup):\n\u001b[32m   1375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._multi_take(tup)\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data_analysis/lib/python3.12/site-packages/pandas/core/indexing.py:1020\u001b[39m, in \u001b[36m_LocationIndexer._getitem_tuple_same_dim\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m com.is_null_slice(key):\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m retval = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m retval.ndim == \u001b[38;5;28mself\u001b[39m.ndim\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data_analysis/lib/python3.12/site-packages/pandas/core/indexing.py:1420\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1418\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index with multidimensional key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data_analysis/lib/python3.12/site-packages/pandas/core/indexing.py:1360\u001b[39m, in \u001b[36m_LocIndexer._getitem_iterable\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m   1359\u001b[39m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m keyarr, indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._reindex_with_indexers(\n\u001b[32m   1362\u001b[39m     {axis: [keyarr, indexer]}, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1363\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data_analysis/lib/python3.12/site-packages/pandas/core/indexing.py:1558\u001b[39m, in \u001b[36m_LocIndexer._get_listlike_indexer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1555\u001b[39m ax = \u001b[38;5;28mself\u001b[39m.obj._get_axis(axis)\n\u001b[32m   1556\u001b[39m axis_name = \u001b[38;5;28mself\u001b[39m.obj._get_axis_name(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1558\u001b[39m keyarr, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data_analysis/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/data_analysis/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"['sector_firm'] not in index\""
          ]
        }
      ],
      "source": [
        "# Filter dataset for Basilicata, Molise, and Puglia with selected columns\n",
        "columns_to_keep = [\"id_worker\", \"year\", \"type\", \"wage\", \"contract_type\", \"sector_12cat\", \"region_res\"]\n",
        "\n",
        "if \"region_res\" not in df.columns:\n",
        "    raise KeyError(\"Column 'region_res' not found in dataframe\")\n",
        "\n",
        "region_filters = {\n",
        "    \"basilicata\": \"2\",\n",
        "    \"molise\": \"12\",\n",
        "    \"puglia\": \"16\",\n",
        "}\n",
        "\n",
        "filtered_subsets = {}\n",
        "region_series = df[\"region_res\"].astype(str)\n",
        "\n",
        "for region_name, region_code in region_filters.items():\n",
        "    mask = region_series == region_code\n",
        "    if mask.any():\n",
        "        subset = df.loc[mask, columns_to_keep].copy()\n",
        "        filtered_subsets[region_name] = subset\n",
        "        print(f\"{region_name.title()} filtered shape: {subset.shape}\")\n",
        "    else:\n",
        "        filtered_subsets[region_name] = pd.DataFrame(columns=columns_to_keep)\n",
        "        print(f\"Warning: no rows found for region code {region_code} ({region_name})\")\n",
        "\n",
        "basilicata_filtered = filtered_subsets[\"basilicata\"]\n",
        "molise_filtered = filtered_subsets[\"molise\"]\n",
        "puglia_filtered = filtered_subsets[\"puglia\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "VERIFICATION: sector_12cat column presence\n",
            "================================================================================\n",
            "Basilicata: sector_12cat present = True\n",
            "  Columns: ['id_worker', 'year', 'type', 'wage', 'contract_type', 'sector_12cat', 'region_res']\n",
            "  sector_12cat non-null count: 193,599 / 555,155\n",
            "  Unique sector_12cat values: ['1', '10', '2', '3', '4', '5', '6', '7', '8', '9']\n",
            "\n",
            "Molise: sector_12cat present = True\n",
            "  Columns: ['id_worker', 'year', 'type', 'wage', 'contract_type', 'sector_12cat', 'region_res']\n",
            "  sector_12cat non-null count: 104,316 / 298,889\n",
            "  Unique sector_12cat values: ['1', '10', '2', '3', '4', '5', '6', '7', '8', '9']\n",
            "\n",
            "Puglia: No data\n"
          ]
        }
      ],
      "source": [
        "# Verify that sector_12cat column is present in filtered datasets\n",
        "print(\"=\" * 80)\n",
        "print(\"VERIFICATION: sector_12cat column presence\")\n",
        "print(\"=\" * 80)\n",
        "for region_name, subset in {\n",
        "    \"basilicata\": basilicata_filtered,\n",
        "    \"molise\": molise_filtered,\n",
        "    \"puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    if subset.empty:\n",
        "        print(f\"{region_name.title()}: No data\")\n",
        "    else:\n",
        "        has_sector_12cat = \"sector_12cat\" in subset.columns\n",
        "        print(f\"{region_name.title()}: sector_12cat present = {has_sector_12cat}\")\n",
        "        if has_sector_12cat:\n",
        "            print(f\"  Columns: {list(subset.columns)}\")\n",
        "            print(f\"  sector_12cat non-null count: {subset['sector_12cat'].notna().sum():,} / {len(subset):,}\")\n",
        "            # Show unique values\n",
        "            unique_vals = subset['sector_12cat'].dropna().unique()\n",
        "            print(f\"  Unique sector_12cat values: {sorted([str(v) for v in unique_vals])}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type value counts (overall):\n",
            "type\n",
            "1    354897\n",
            "2     52545\n",
            "3     48611\n",
            "4    397991\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Type labels from metadata:\n",
            "{1: 'Private employees', 2: 'Public employees', 3: 'Self-employed', 4: 'Non employed'}\n"
          ]
        }
      ],
      "source": [
        "# Inspect mapping of 'type' to identify non-employed code\n",
        "print(\"Type value counts (overall):\")\n",
        "print(df[\"type\"].value_counts().sort_index())\n",
        "\n",
        "if 'type' in meta.variable_value_labels:\n",
        "    print(\"\\nType labels from metadata:\")\n",
        "    print(meta.variable_value_labels['type'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "basilicata employed share: 0.5292\n",
            "molise employed share: 0.5428\n",
            "puglia employed share: n/a (no data)\n"
          ]
        }
      ],
      "source": [
        "# Create binary employed indicator\n",
        "regional_sets = {\n",
        "    \"basilicata\": basilicata_filtered,\n",
        "    \"molise\": molise_filtered,\n",
        "    \"puglia\": puglia_filtered,\n",
        "}\n",
        "\n",
        "for region_name, subset in regional_sets.items():\n",
        "    if subset.empty:\n",
        "        print(f\"{region_name} employed share: n/a (no data)\")\n",
        "        continue\n",
        "    wage_positive = pd.to_numeric(subset[\"wage\"], errors=\"coerce\").fillna(0) > 0\n",
        "    type_non_employed = subset[\"type\"].eq(4)\n",
        "    subset[\"employed\"] = (wage_positive | ~type_non_employed).astype(\"Int8\")\n",
        "    print(f\"{region_name} employed share: {subset['employed'].mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "basilicata: retained 292,964 rows for 9,875 eligible workers\n",
            "molise: retained 160,799 rows for 5,431 eligible workers\n",
            "puglia: retained 0 rows (no data)\n"
          ]
        }
      ],
      "source": [
        "# Retain only workers employed in 1999, 2000, or 2001\n",
        "COHORT_YEARS = {1999, 2000, 2001}\n",
        "filtered_results = {}\n",
        "\n",
        "for region_name, subset in {\n",
        "    \"basilicata\": basilicata_filtered,\n",
        "    \"molise\": molise_filtered,\n",
        "    \"puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    if subset.empty:\n",
        "        filtered_results[region_name] = subset\n",
        "        print(f\"{region_name}: retained 0 rows (no data)\")\n",
        "        continue\n",
        "\n",
        "    eligible_workers = subset.loc[\n",
        "        subset[\"year\"].isin(COHORT_YEARS) & subset[\"employed\"].eq(1),\n",
        "        \"id_worker\"\n",
        "    ].unique()\n",
        "    filtered_subset = subset[subset[\"id_worker\"].isin(eligible_workers)].copy()\n",
        "    filtered_results[region_name] = filtered_subset\n",
        "    print(\n",
        "        f\"{region_name}: retained {len(filtered_subset):,} rows for\"\n",
        "        f\" {len(eligible_workers):,} eligible workers\"\n",
        "    )\n",
        "\n",
        "basilicata_filtered = filtered_results[\"basilicata\"]\n",
        "molise_filtered = filtered_results[\"molise\"]\n",
        "puglia_filtered = filtered_results[\"puglia\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After year restriction:\n",
            "  Basilicata: 75,905 rows\n",
            "  Molise: 41,610 rows\n",
            "  Puglia: 0 rows\n"
          ]
        }
      ],
      "source": [
        "YEAR_WINDOW = range(1999, 2007)\n",
        "\n",
        "restricted_sets = {}\n",
        "for region_name, subset in {\n",
        "    \"Basilicata\": basilicata_filtered,\n",
        "    \"Molise\": molise_filtered,\n",
        "    \"Puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    restricted = subset[subset[\"year\"].isin(YEAR_WINDOW)].copy()\n",
        "    restricted_sets[region_name.lower()] = restricted\n",
        "\n",
        "print(\"After year restriction:\")\n",
        "for region_name, subset in restricted_sets.items():\n",
        "    print(f\"  {region_name.title()}: {len(subset):,} rows\")\n",
        "\n",
        "basilicata_filtered = restricted_sets[\"basilicata\"]\n",
        "molise_filtered = restricted_sets[\"molise\"]\n",
        "puglia_filtered = restricted_sets[\"puglia\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CREATING sector_12cat_cont COLUMN (latest sector_12cat per worker, per year)\n",
            "================================================================================\n",
            "\n",
            "Basilicata:\n",
            "  Total rows: 75,905\n",
            "  Rows with original sector_12cat: 43,851\n",
            "  Rows with sector_12cat_cont: 55,970\n",
            "  Rows filled (had NaN, now have value): 12,119\n",
            "\n",
            "  Year-by-year breakdown:\n",
            "    1999: 9,732 rows | 5,412 with original sector_12cat | 5,412 with sector_12cat_cont | 0 filled\n",
            "    2000: 9,753 rows | 5,548 with original sector_12cat | 6,429 with sector_12cat_cont | 881 filled\n",
            "    2001: 9,706 rows | 6,050 with original sector_12cat | 7,299 with sector_12cat_cont | 1,249 filled\n",
            "    2002: 9,590 rows | 5,648 with original sector_12cat | 7,321 with sector_12cat_cont | 1,673 filled\n",
            "    2003: 9,475 rows | 5,355 with original sector_12cat | 7,278 with sector_12cat_cont | 1,923 filled\n",
            "    2004: 9,350 rows | 5,132 with original sector_12cat | 7,237 with sector_12cat_cont | 2,105 filled\n",
            "    2005: 9,216 rows | 5,435 with original sector_12cat | 7,535 with sector_12cat_cont | 2,100 filled\n",
            "    2006: 9,083 rows | 5,271 with original sector_12cat | 7,459 with sector_12cat_cont | 2,188 filled\n",
            "\n",
            "Molise:\n",
            "  Total rows: 41,610\n",
            "  Rows with original sector_12cat: 23,275\n",
            "  Rows with sector_12cat_cont: 29,698\n",
            "  Rows filled (had NaN, now have value): 6,423\n",
            "\n",
            "  Year-by-year breakdown:\n",
            "    1999: 5,360 rows | 2,883 with original sector_12cat | 2,883 with sector_12cat_cont | 0 filled\n",
            "    2000: 5,366 rows | 3,011 with original sector_12cat | 3,424 with sector_12cat_cont | 413 filled\n",
            "    2001: 5,333 rows | 3,224 with original sector_12cat | 3,874 with sector_12cat_cont | 650 filled\n",
            "    2002: 5,260 rows | 2,962 with original sector_12cat | 3,885 with sector_12cat_cont | 923 filled\n",
            "    2003: 5,185 rows | 2,805 with original sector_12cat | 3,867 with sector_12cat_cont | 1,062 filled\n",
            "    2004: 5,112 rows | 2,712 with original sector_12cat | 3,841 with sector_12cat_cont | 1,129 filled\n",
            "    2005: 5,033 rows | 2,878 with original sector_12cat | 3,981 with sector_12cat_cont | 1,103 filled\n",
            "    2006: 4,961 rows | 2,800 with original sector_12cat | 3,943 with sector_12cat_cont | 1,143 filled\n",
            "\n",
            "Puglia: No data - skipping\n"
          ]
        }
      ],
      "source": [
        "# Create sector_12cat_cont: assign each worker the latest seen sector_12cat value up to each year\n",
        "# This handles panel data gaps by forward-filling the most recent sector for each worker\n",
        "print(\"=\" * 80)\n",
        "print(\"CREATING sector_12cat_cont COLUMN (latest sector_12cat per worker, per year)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for region_name, subset in {\n",
        "    \"Basilicata\": basilicata_filtered,\n",
        "    \"Molise\": molise_filtered,\n",
        "    \"Puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    if subset.empty:\n",
        "        print(f\"\\n{region_name}: No data - skipping\")\n",
        "        continue\n",
        "    \n",
        "    # Sort by worker and year to ensure chronological order\n",
        "    subset_sorted = subset.sort_values([\"id_worker\", \"year\"]).copy()\n",
        "    \n",
        "    # For each worker, forward-fill the sector_12cat (carry forward latest known value up to each year)\n",
        "    # This means if a worker was in \"2\" in 2000, and we don't have data for 2001,\n",
        "    # we assign \"2\" to them in 2001 (using the latest known value up to that year)\n",
        "    subset_sorted[\"sector_12cat_cont\"] = subset_sorted.groupby(\"id_worker\")[\"sector_12cat\"].ffill()\n",
        "    \n",
        "    # Update the original filtered dataframe\n",
        "    if region_name == \"Basilicata\":\n",
        "        basilicata_filtered = subset_sorted.sort_index()\n",
        "    elif region_name == \"Molise\":\n",
        "        molise_filtered = subset_sorted.sort_index()\n",
        "    elif region_name == \"Puglia\":\n",
        "        puglia_filtered = subset_sorted.sort_index()\n",
        "    \n",
        "    # Print overall diagnostics\n",
        "    print(f\"\\n{region_name}:\")\n",
        "    print(f\"  Total rows: {len(subset_sorted):,}\")\n",
        "    print(f\"  Rows with original sector_12cat: {subset_sorted['sector_12cat'].notna().sum():,}\")\n",
        "    print(f\"  Rows with sector_12cat_cont: {subset_sorted['sector_12cat_cont'].notna().sum():,}\")\n",
        "    print(f\"  Rows filled (had NaN, now have value): {(subset_sorted['sector_12cat_cont'].notna() & subset_sorted['sector_12cat'].isna()).sum():,}\")\n",
        "    \n",
        "    # Print year-by-year breakdown\n",
        "    print(f\"\\n  Year-by-year breakdown:\")\n",
        "    for year in sorted(subset_sorted[\"year\"].unique()):\n",
        "        year_data = subset_sorted[subset_sorted[\"year\"] == year]\n",
        "        total_rows = len(year_data)\n",
        "        with_original = year_data[\"sector_12cat\"].notna().sum()\n",
        "        with_cont = year_data[\"sector_12cat_cont\"].notna().sum()\n",
        "        filled = (year_data[\"sector_12cat_cont\"].notna() & year_data[\"sector_12cat\"].isna()).sum()\n",
        "        print(f\"    {year}: {total_rows:,} rows | {with_original:,} with original sector_12cat | {with_cont:,} with sector_12cat_cont | {filled:,} filled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PEOPLE WORKING IN SECTOR_12CAT == '2' BY YEAR\n",
            "================================================================================\n",
            "\n",
            "Basilicata: No workers in sector_12cat == '2'\n",
            "\n",
            "Molise: No workers in sector_12cat == '2'\n",
            "\n",
            "Puglia: No data\n"
          ]
        }
      ],
      "source": [
        "# Count how many people work in sector_12cat == \"2\" each year\n",
        "print(\"=\" * 80)\n",
        "print(\"PEOPLE WORKING IN SECTOR_12CAT == '2' BY YEAR\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for region_name, subset in {\n",
        "    \"Basilicata\": basilicata_filtered,\n",
        "    \"Molise\": molise_filtered,\n",
        "    \"Puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    if subset.empty:\n",
        "        print(f\"\\n{region_name}: No data\")\n",
        "        continue\n",
        "    \n",
        "    # Filter for sector_12cat == 2 (handle both numeric and string)\n",
        "    # Convert to string for comparison to handle both numeric 2 and string \"2\"\n",
        "    sector_2_mask = subset[\"sector_12cat\"].astype(str) == \"2\"\n",
        "    \n",
        "    if not sector_2_mask.any():\n",
        "        print(f\"\\n{region_name}: No workers in sector_12cat == 2\")\n",
        "        continue\n",
        "    \n",
        "    # Count unique workers per year in sector 2\n",
        "    sector_2_data = subset[sector_2_mask].copy()\n",
        "    workers_by_year = sector_2_data.groupby(\"year\")[\"id_worker\"].nunique().sort_index()\n",
        "    \n",
        "    print(f\"\\n{region_name}:\")\n",
        "    print(f\"  Total rows with sector_12cat == '2': {len(sector_2_data):,}\")\n",
        "    print(f\"  Unique workers in sector 2 by year:\")\n",
        "    for year, count in workers_by_year.items():\n",
        "        print(f\"    {year}: {count:,} unique workers\")\n",
        "    \n",
        "    # Also show total count across all years\n",
        "    total_unique_workers = sector_2_data[\"id_worker\"].nunique()\n",
        "    print(f\"  Total unique workers (across all years): {total_unique_workers:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "COMPARISON: sector_12cat vs sector_12cat_cont for '2'\n",
            "================================================================================\n",
            "\n",
            "Basilicata:\n",
            "  Rows with sector_12cat == '2': 0\n",
            "  Rows with sector_12cat_cont == '2': 0\n",
            "  Difference (filled): 0 additional rows\n",
            "\n",
            "  Unique workers by year (using sector_12cat_cont):\n",
            "\n",
            "Molise:\n",
            "  Rows with sector_12cat == '2': 0\n",
            "  Rows with sector_12cat_cont == '2': 0\n",
            "  Difference (filled): 0 additional rows\n",
            "\n",
            "  Unique workers by year (using sector_12cat_cont):\n",
            "\n",
            "Puglia: No data\n"
          ]
        }
      ],
      "source": [
        "# Compare sector_12cat vs sector_12cat_cont for sector 2\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPARISON: sector_12cat vs sector_12cat_cont for '2'\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for region_name, subset in {\n",
        "    \"Basilicata\": basilicata_filtered,\n",
        "    \"Molise\": molise_filtered,\n",
        "    \"Puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    if subset.empty:\n",
        "        print(f\"\\n{region_name}: No data\")\n",
        "        continue\n",
        "    \n",
        "    # Count using original sector_12cat (handle both numeric and string)\n",
        "    sector_2_original = (subset[\"sector_12cat\"].astype(str) == \"2\").sum()\n",
        "    workers_original = subset[subset[\"sector_12cat\"].astype(str) == \"2\"].groupby(\"year\")[\"id_worker\"].nunique().sort_index()\n",
        "    \n",
        "    # Count using sector_12cat_cont (handle both numeric and string)\n",
        "    sector_2_cont = (subset[\"sector_12cat_cont\"].astype(str) == \"2\").sum()\n",
        "    workers_cont = subset[subset[\"sector_12cat_cont\"].astype(str) == \"2\"].groupby(\"year\")[\"id_worker\"].nunique().sort_index()\n",
        "    \n",
        "    print(f\"\\n{region_name}:\")\n",
        "    print(f\"  Rows with sector_12cat == '2': {sector_2_original:,}\")\n",
        "    print(f\"  Rows with sector_12cat_cont == '2': {sector_2_cont:,}\")\n",
        "    print(f\"  Difference (filled): {sector_2_cont - sector_2_original:,} additional rows\")\n",
        "    print(f\"\\n  Unique workers by year (using sector_12cat_cont):\")\n",
        "    for year, count in workers_cont.items():\n",
        "        orig_count = workers_original.get(year, 0)\n",
        "        diff = count - orig_count\n",
        "        print(f\"    {year}: {count:,} unique workers (original: {orig_count:,}, +{diff:,})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TRANSITIONS AND PERSISTENCE IN '2' (using sector_12cat_cont)\n",
            "================================================================================\n",
            "\n",
            "Basilicata: No workers in sector_12cat_cont == '2'\n",
            "\n",
            "Molise: No workers in sector_12cat_cont == '2'\n",
            "\n",
            "Puglia: No data\n"
          ]
        }
      ],
      "source": [
        "# Analyze transitions and persistence in sector 2 using sector_12cat_cont\n",
        "print(\"=\" * 80)\n",
        "print(\"TRANSITIONS AND PERSISTENCE IN '2' (using sector_12cat_cont)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for region_name, subset in {\n",
        "    \"Basilicata\": basilicata_filtered,\n",
        "    \"Molise\": molise_filtered,\n",
        "    \"Puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    if subset.empty:\n",
        "        print(f\"\\n{region_name}: No data\")\n",
        "        continue\n",
        "    \n",
        "    # Filter for sector_12cat_cont == 2 (handle both numeric and string)\n",
        "    sector_2_data = subset[subset[\"sector_12cat_cont\"].astype(str) == \"2\"].copy()\n",
        "    \n",
        "    if len(sector_2_data) == 0:\n",
        "        print(f\"\\n{region_name}: No workers in sector_12cat_cont == 2\")\n",
        "        continue\n",
        "    \n",
        "    # Get unique workers by year\n",
        "    workers_by_year = {}\n",
        "    for year in sorted(sector_2_data[\"year\"].unique()):\n",
        "        workers_by_year[year] = set(sector_2_data[sector_2_data[\"year\"] == year][\"id_worker\"].unique())\n",
        "    \n",
        "    print(f\"\\n{region_name}:\")\n",
        "    print(f\"  Analysis of workers in sector_12cat_cont == '2':\")\n",
        "    \n",
        "    # Analyze year by year\n",
        "    years = sorted(workers_by_year.keys())\n",
        "    for i, year in enumerate(years):\n",
        "        current_workers = workers_by_year[year]\n",
        "        print(f\"\\n    {year}: {len(current_workers):,} unique workers\")\n",
        "        \n",
        "        if i > 0:\n",
        "            prev_year = years[i-1]\n",
        "            prev_workers = workers_by_year[prev_year]\n",
        "            \n",
        "            # Workers who continued from previous year\n",
        "            continuing = current_workers & prev_workers\n",
        "            # New entrants (in current year but not in previous)\n",
        "            entrants = current_workers - prev_workers\n",
        "            # Exits (in previous year but not in current)\n",
        "            exits = prev_workers - current_workers\n",
        "            \n",
        "            print(f\"      Continuing from {prev_year}: {len(continuing):,} workers\")\n",
        "            print(f\"      New entrants: {len(entrants):,} workers\")\n",
        "            if len(entrants) > 0 and len(entrants) <= 10:\n",
        "                print(f\"        Worker IDs: {sorted(list(entrants))}\")\n",
        "            print(f\"      Exits (left sector 2): {len(exits):,} workers\")\n",
        "            if len(exits) > 0 and len(exits) <= 10:\n",
        "                print(f\"        Worker IDs: {sorted(list(exits))}\")\n",
        "    \n",
        "    # Find workers who appear in multiple consecutive years\n",
        "    print(f\"\\n  Persistence analysis:\")\n",
        "    worker_years = sector_2_data.groupby(\"id_worker\")[\"year\"].apply(lambda x: sorted(set(x))).to_dict()\n",
        "    \n",
        "    # Count workers by how many years they appear\n",
        "    years_count = {}\n",
        "    for worker, years_list in worker_years.items():\n",
        "        num_years = len(years_list)\n",
        "        if num_years not in years_count:\n",
        "            years_count[num_years] = []\n",
        "        years_count[num_years].append((worker, years_list))\n",
        "    \n",
        "    for num_years in sorted(years_count.keys(), reverse=True):\n",
        "        workers_list = years_count[num_years]\n",
        "        print(f\"    Workers appearing in {num_years} year(s): {len(workers_list):,}\")\n",
        "        if len(workers_list) <= 5:\n",
        "            for worker, years_list in workers_list:\n",
        "                print(f\"      Worker {worker}: years {years_list}\")\n",
        "    \n",
        "    # Show if same worker appears across multiple years (like 1999-2002)\n",
        "    print(f\"\\n  Workers appearing in consecutive years:\")\n",
        "    consecutive_workers = []\n",
        "    for worker, years_list in worker_years.items():\n",
        "        if len(years_list) > 1:\n",
        "            # Check if years are consecutive\n",
        "            years_sorted = sorted(years_list)\n",
        "            is_consecutive = all(years_sorted[i] == years_sorted[i-1] + 1 for i in range(1, len(years_sorted)))\n",
        "            if is_consecutive:\n",
        "                consecutive_workers.append((worker, years_list))\n",
        "    \n",
        "    consecutive_workers.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    for worker, years_list in consecutive_workers[:10]:  # Show top 10\n",
        "        print(f\"    Worker {worker}: {years_list[0]}-{years_list[-1]} ({len(years_list)} years)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wage min/max by region:\n",
            "  Basilicata: min=0.00, max=163,300.00\n",
            "  Molise: min=0.00, max=175,300.00\n",
            "  Puglia: n/a\n"
          ]
        }
      ],
      "source": [
        "# Wage range diagnostics\n",
        "print(\"Wage min/max by region:\")\n",
        "for region_name, subset in {\n",
        "    \"Basilicata\": basilicata_filtered,\n",
        "    \"Molise\": molise_filtered,\n",
        "    \"Puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    if subset.empty or \"wage\" not in subset.columns:\n",
        "        print(f\"  {region_name}: n/a\")\n",
        "        continue\n",
        "    wages = pd.to_numeric(subset[\"wage\"], errors=\"coerce\")\n",
        "    print(\n",
        "        f\"  {region_name}: min={wages.min(skipna=True):,.2f},\"\n",
        "        f\" max={wages.max(skipna=True):,.2f}\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wage diagnostics before income_category creation:\n",
            "  Basilicata:\n",
            "    Total rows: 75,905\n",
            "    Non-null wages: 62,426\n",
            "    Null wages: 13,479\n",
            "    Wages > 0: 62,343\n",
            "    Wages == 0: 83\n",
            "    Min wage: 0.00\n",
            "    Max wage: 163,300.00\n",
            "    Mean wage: 15,826.99\n",
            "\n",
            "  Molise:\n",
            "    Total rows: 41,610\n",
            "    Non-null wages: 34,302\n",
            "    Null wages: 7,308\n",
            "    Wages > 0: 34,210\n",
            "    Wages == 0: 92\n",
            "    Min wage: 0.00\n",
            "    Max wage: 175,300.00\n",
            "    Mean wage: 16,172.84\n",
            "\n",
            "  Puglia: n/a\n"
          ]
        }
      ],
      "source": [
        "# Diagnostic: Check wage values before creating income_category\n",
        "print(\"Wage diagnostics before income_category creation:\")\n",
        "for region_name, subset in {\n",
        "    \"Basilicata\": basilicata_filtered,\n",
        "    \"Molise\": molise_filtered,\n",
        "    \"Puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    if subset.empty or \"wage\" not in subset.columns:\n",
        "        print(f\"  {region_name}: n/a\")\n",
        "        continue\n",
        "    wages = pd.to_numeric(subset[\"wage\"], errors=\"coerce\")\n",
        "    print(f\"  {region_name}:\")\n",
        "    print(f\"    Total rows: {len(subset):,}\")\n",
        "    print(f\"    Non-null wages: {wages.notna().sum():,}\")\n",
        "    print(f\"    Null wages: {wages.isna().sum():,}\")\n",
        "    print(f\"    Wages > 0: {(wages > 0).sum():,}\")\n",
        "    print(f\"    Wages == 0: {(wages == 0).sum():,}\")\n",
        "    if wages.notna().any():\n",
        "        print(f\"    Min wage: {wages.min():,.2f}\")\n",
        "        print(f\"    Max wage: {wages.max():,.2f}\")\n",
        "        print(f\"    Mean wage: {wages.mean():,.2f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Basilicata: income_category distribution:\n",
            "income_category\n",
            "1    56580\n",
            "2     4713\n",
            "3     1133\n",
            "Name: count, dtype: Int64\n",
            "  Basilicata: NaN count in income_category: 13,479\n",
            "\n",
            "  Molise: income_category distribution:\n",
            "income_category\n",
            "1    30860\n",
            "2     2836\n",
            "3      606\n",
            "Name: count, dtype: Int64\n",
            "  Molise: NaN count in income_category: 7,308\n",
            "\n",
            "  Puglia: n/a (no data or no wage column)\n"
          ]
        }
      ],
      "source": [
        "# Add income_category column based on wage\n",
        "for region_name, subset in {\n",
        "    \"Basilicata\": basilicata_filtered,\n",
        "    \"Molise\": molise_filtered,\n",
        "    \"Puglia\": puglia_filtered,\n",
        "}.items():\n",
        "    if subset.empty or \"wage\" not in subset.columns:\n",
        "        print(f\"  {region_name}: n/a (no data or no wage column)\")\n",
        "        continue\n",
        "    \n",
        "    # Convert wage to numeric\n",
        "    wages = pd.to_numeric(subset[\"wage\"], errors=\"coerce\")\n",
        "    \n",
        "    # Initialize income_category column with NaN\n",
        "    income_cat = pd.Series(index=subset.index, dtype=\"Int8\")\n",
        "    \n",
        "    # Only assign categories for non-NaN wages\n",
        "    valid_mask = wages.notna()\n",
        "    if valid_mask.any():\n",
        "        valid_wages = wages[valid_mask]\n",
        "        \n",
        "        # Create income category column (using numeric codes: 1=low, 2=medium, 3=high)\n",
        "        conditions = [\n",
        "            valid_wages < 28000,\n",
        "            (valid_wages >= 28000) & (valid_wages <= 50000),\n",
        "            valid_wages > 50000\n",
        "        ]\n",
        "        choices = [1, 2, 3]  # 1=low income, 2=medium, 3=high\n",
        "        \n",
        "        # Assign categories only for valid wages\n",
        "        income_cat[valid_mask] = np.select(conditions, choices, default=None)\n",
        "    \n",
        "    # Assign to the subset DataFrame\n",
        "    subset[\"income_category\"] = income_cat.astype(\"Int8\")\n",
        "    \n",
        "    print(f\"  {region_name}: income_category distribution:\")\n",
        "    print(subset[\"income_category\"].value_counts().sort_index())\n",
        "    print(f\"  {region_name}: NaN count in income_category: {subset['income_category'].isna().sum():,}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved Basilicata filtered data to /Users/vittoriogaravelli/GitHub/GitHub/ERMDA-30464_GP/data/derived/basilicata_filtered.dta\n",
            "Saved Molise filtered data to /Users/vittoriogaravelli/GitHub/GitHub/ERMDA-30464_GP/data/derived/molise_filtered.dta\n",
            "Skipping puglia export (no data)\n"
          ]
        }
      ],
      "source": [
        "# Save filtered datasets\n",
        "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "output_specs = {\n",
        "    \"basilicata\": (basilicata_filtered, DERIVED_DIR / \"basilicata_filtered.dta\"),\n",
        "    \"molise\": (molise_filtered, DERIVED_DIR / \"molise_filtered.dta\"),\n",
        "    \"puglia\": (puglia_filtered, DERIVED_DIR / \"puglia_filtered.dta\"),\n",
        "}\n",
        "\n",
        "for region_name, (subset, path) in output_specs.items():\n",
        "    if subset.empty:\n",
        "        print(f\"Skipping {region_name} export (no data)\")\n",
        "        continue\n",
        "    subset.convert_dtypes().to_stata(path, write_index=False, version=118)\n",
        "    print(f\"Saved {region_name.title()} filtered data to {path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data_analysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
